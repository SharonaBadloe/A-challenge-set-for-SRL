{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Hq2e8-ah0B55",
    "outputId": "36aaecc8-46d7-42c9-a78c-40a13ec070f2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# installations (uncomment if necessary)\n",
    "\n",
    "# !pip install checklist\n",
    "# !pip install allennlp\n",
    "# !pip install allennlp_models\n",
    "# !pip install --upgrade google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I7SCi6jR0G_O",
    "outputId": "68560f86-5799-4d40-d52e-de298958ef4c"
   },
   "outputs": [],
   "source": [
    "# imports (uncomment nltk download if necessary)\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import checklist\n",
    "from checklist.editor import Editor\n",
    "from checklist.perturb import Perturb\n",
    "from checklist.test_types import MFT, INV, DIR\n",
    "from checklist.expect import Expect\n",
    "from checklist.pred_wrapper import PredictorWrapper\n",
    "from checklist.test_suite import TestSuite\n",
    "\n",
    "from allennlp_models.pretrained import load_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "yuw8JtD00eZ4",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "daa6172e-e622-415b-f31b-1688ea08edec",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\coref-spanbert.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\evaluate_rc-lerc.json as plain json\n",
      "lerc is not a registered model.\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\generation-bart.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\glove-sst.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\lm-masked-language-model.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\lm-next-token-lm-gpt2.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\mc-roberta-commonsenseqa.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\mc-roberta-piqa.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\mc-roberta-swag.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\nlvr2-vilbert-head.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\nlvr2-vilbert.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\pair-classification-adversarial-binary-gender-bias-mitigated-roberta-snli.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\pair-classification-binary-gender-bias-mitigated-roberta-snli.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\pair-classification-decomposable-attention-elmo.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\pair-classification-esim.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\pair-classification-roberta-mnli.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\pair-classification-roberta-rte.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\pair-classification-roberta-snli.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\rc-bidaf-elmo.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\rc-bidaf.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\rc-naqanet.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\rc-nmn.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\rc-transformer-qa.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\roberta-sst.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\semparse-nlvr.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\semparse-text-to-sql.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\semparse-wikitables.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\structured-prediction-biaffine-parser.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\structured-prediction-constituency-parser.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\structured-prediction-srl-bert.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\structured-prediction-srl.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\tagging-elmo-crf-tagger.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\tagging-fine-grained-crf-tagger.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\tagging-fine-grained-transformer-crf-tagger.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\ve-vilbert.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\vgqa-vilbert.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\vqa-vilbert.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\AppData\\Local\\Temp\\tmp0cfxl4fe\\config.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\coref-spanbert.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\evaluate_rc-lerc.json as plain json\n",
      "lerc is not a registered model.\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\generation-bart.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\glove-sst.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\lm-masked-language-model.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\lm-next-token-lm-gpt2.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\mc-roberta-commonsenseqa.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\mc-roberta-piqa.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\mc-roberta-swag.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\nlvr2-vilbert-head.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\nlvr2-vilbert.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\pair-classification-adversarial-binary-gender-bias-mitigated-roberta-snli.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\pair-classification-binary-gender-bias-mitigated-roberta-snli.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\pair-classification-decomposable-attention-elmo.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\pair-classification-esim.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\pair-classification-roberta-mnli.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\pair-classification-roberta-rte.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\pair-classification-roberta-snli.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\rc-bidaf-elmo.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\rc-bidaf.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\rc-naqanet.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\rc-nmn.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\rc-transformer-qa.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\roberta-sst.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\semparse-nlvr.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\semparse-text-to-sql.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\semparse-wikitables.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\structured-prediction-biaffine-parser.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\structured-prediction-constituency-parser.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\structured-prediction-srl-bert.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\structured-prediction-srl.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\tagging-elmo-crf-tagger.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\tagging-fine-grained-crf-tagger.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\tagging-fine-grained-transformer-crf-tagger.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\ve-vilbert.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\vgqa-vilbert.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\anaconda3\\Lib\\site-packages\\allennlp_models\\modelcards\\vqa-vilbert.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Shark\\AppData\\Local\\Temp\\tmp0udl0k31\\config.json as plain json\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# load models\n",
    "\n",
    "srl_predictor = load_predictor('structured-prediction-srl')\n",
    "srl_predictor_bert = load_predictor('structured-prediction-srl-bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "7z21V1x80oAq"
   },
   "outputs": [],
   "source": [
    "# get srl model predictions into PredictorWrapper format\n",
    "\n",
    "### added by pia ###\n",
    "\n",
    "def predict_srl(data):\n",
    "    \n",
    "    pred = []\n",
    "    for d in data:\n",
    "        pred.append(srl_predictor.predict(d))\n",
    "    return pred\n",
    "\n",
    "wrapper_srl = PredictorWrapper.wrap_predict(predict_srl)\n",
    "\n",
    "def predict_bert(data):\n",
    "    \n",
    "    pred = []\n",
    "    for d in data:\n",
    "        pred.append(srl_predictor_bert.predict(d))\n",
    "    return pred\n",
    "\n",
    "wrapper_bert = PredictorWrapper.wrap_predict(predict_bert)\n",
    "\n",
    "# summary format functions\n",
    "\n",
    "def format_srl_last(x, pred, conf, label=None, meta=None):\n",
    "    results = []\n",
    "    predicate_structure = pred['verbs'][-1]['description']\n",
    "        \n",
    "    return predicate_structure\n",
    "\n",
    "def format_srl_first(x, pred, conf, label=None, meta=None):\n",
    "    results = []\n",
    "    predicate_structure = pred['verbs'][0]['description']\n",
    "        \n",
    "    return predicate_structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get target arg functions\n",
    "\n",
    "def get_argTMP(pred, arg_target='ARGM-TMP'):\n",
    "    # we assume one predicate:\n",
    "    predicate_arguments = pred['verbs'][0]\n",
    "    words = pred['words']\n",
    "    tags = predicate_arguments['tags']\n",
    "    \n",
    "    arg_list = []\n",
    "    for t, w in zip(tags, words):\n",
    "        if t.endswith(arg_target):\n",
    "            arg_list.append(w)\n",
    "    found_arg = ' '.join(arg_list)\n",
    "                         \n",
    "    return found_arg\n",
    "\n",
    "def get_arg2(pred, arg_target='ARG2'):\n",
    "    # we assume one predicate:\n",
    "    predicate_arguments = pred['verbs'][0]\n",
    "    words = pred['words']\n",
    "    tags = predicate_arguments['tags']\n",
    "    \n",
    "    arg_list = []\n",
    "    for t, w in zip(tags, words):\n",
    "        if t.endswith(arg_target):\n",
    "            arg_list.append(w)\n",
    "    found_arg = ' '.join(arg_list)\n",
    "                         \n",
    "    return found_arg\n",
    "\n",
    "def get_dative_args(pred):\n",
    "    # we assume one predicate:\n",
    "    predicate_arguments = pred['verbs'][0]\n",
    "    words = pred['words']\n",
    "    tags = predicate_arguments['tags']\n",
    "    \n",
    "    arg0_list = []\n",
    "    arg1_list = []\n",
    "    arg2_list = []\n",
    "    \n",
    "    for t, w in zip(tags, words):\n",
    "        if t.endswith('ARG0'):\n",
    "            arg0_list.append(w)\n",
    "    found_arg0 = ' '.join(arg0_list)\n",
    "    \n",
    "    for t, w in zip(tags, words):\n",
    "        if t.endswith('ARG1'):\n",
    "            arg1_list.append(w)\n",
    "    found_arg1 = ' '.join(arg1_list)\n",
    "    \n",
    "    for t, w in zip(tags, words):\n",
    "        if t.endswith('ARG2'):\n",
    "            arg2_list.append(w)\n",
    "    found_arg2 = ' '.join(arg2_list)\n",
    "                         \n",
    "    return found_arg0, found_arg1, found_arg2\n",
    "            \n",
    "def get_arg_span_first(pred, target_span=[]):\n",
    "    # we assume one predicate:\n",
    "    predicate_arguments = pred['verbs'][0]\n",
    "    words = pred['words']\n",
    "    tags = predicate_arguments['tags']\n",
    "    \n",
    "    arg_list = []\n",
    "    for t, w in zip(tags, words):\n",
    "        arg = t\n",
    "        if '-' in t:\n",
    "            arg = t.split('-')[1]\n",
    "        if w in target_span:\n",
    "            arg_list.append(arg)\n",
    "    return arg_list\n",
    "\n",
    "def get_arg_span_last(pred, target_span=[]):\n",
    "    # we assume one predicate:\n",
    "    predicate_arguments = pred['verbs'][-1]\n",
    "    words = pred['words']\n",
    "    tags = predicate_arguments['tags']\n",
    "    \n",
    "    arg_list = []\n",
    "    for t, w in zip(tags, words):\n",
    "        arg = t\n",
    "        if '-' in t:\n",
    "            arg = t.split('-')[1]\n",
    "        if w in target_span:\n",
    "            arg_list.append(arg)\n",
    "    return arg_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expectation functions\n",
    "\n",
    "def expect_argTMP(x, pred, conf, label=None, meta=None):\n",
    "    \n",
    "    # people should be recognized as arg1\n",
    "\n",
    "    arg = get_argTMP(pred)\n",
    "\n",
    "    if arg == 'last week':\n",
    "        pass_ = True\n",
    "    else:\n",
    "        pass_ = False\n",
    "    return pass_\n",
    "\n",
    "def expect_arg2(x, pred, conf, label=None, meta=None):\n",
    "    \n",
    "    # people should be recognized as arg1\n",
    "\n",
    "    arg = get_arg2(pred)\n",
    "\n",
    "    if arg == 'with a knife':\n",
    "        pass_ = True\n",
    "    else:\n",
    "        pass_ = False\n",
    "    return pass_\n",
    "\n",
    "def expect_dativeIOC(x, pred, conf, label=None, meta=None):\n",
    "    \n",
    "    # should be recognized as arg\n",
    "    arg0, arg1, arg2 = get_dative_args(pred)\n",
    "    mask = ' '.join(meta['mask'])\n",
    "\n",
    "    if arg0 == meta['first_name1'] and arg1 == mask and arg2 == 'to ' + meta['first_name2']:\n",
    "        pass_ = True\n",
    "    else:\n",
    "        pass_ = False\n",
    "    return pass_\n",
    "\n",
    "def expect_dativeDOC(x, pred, conf, label=None, meta=None):\n",
    "    \n",
    "    # should be recognized as arg\n",
    "    arg0, arg1, arg2 = get_dative_args(pred)\n",
    "    mask = ' '.join(meta['mask'])\n",
    "\n",
    "    if arg0 == meta['first_name1'] and arg1 == mask and arg2 == meta['first_name2']:\n",
    "        pass_ = True\n",
    "    else:\n",
    "        pass_ = False\n",
    "    return pass_\n",
    "\n",
    "def compare_spans(orig_pred, pred, orig_conf, conf, labels=None, meta=None):\n",
    "    \n",
    "    sp_orig = meta['mask1']\n",
    "    sp_pred = meta['mask1']\n",
    "    \n",
    "    l_orig = set(get_arg_span_first(orig_pred, sp_orig))\n",
    "    l_pred = set(get_arg_span_first(pred, sp_pred))\n",
    "    \n",
    "    if l_orig == l_pred:\n",
    "        pass_ = True\n",
    "    else:\n",
    "        pass_ = False\n",
    "    \n",
    "    \n",
    "    return pass_\n",
    "\n",
    "def compare_voice(orig_pred, pred, orig_conf, conf, labels=None, meta=None):\n",
    "    \n",
    "    sp_orig = meta['mask1']\n",
    "    sp_pred = meta['mask1']\n",
    "    \n",
    "    l_orig = set(get_arg_span_last(orig_pred, sp_orig))\n",
    "    l_pred = set(get_arg_span_first(pred, sp_pred))\n",
    "    \n",
    "    if l_orig == l_pred:\n",
    "        pass_ = True\n",
    "    else:\n",
    "        pass_ = False\n",
    "    \n",
    "    \n",
    "    return pass_\n",
    "\n",
    "expectTMP = Expect.single(expect_argTMP)\n",
    "expect2 = Expect.single(expect_arg2)\n",
    "expectIOC = Expect.single(expect_dativeIOC)\n",
    "expectDOC = Expect.single(expect_dativeDOC)\n",
    "expectSpan = Expect.pairwise(compare_spans)\n",
    "expectVoice = Expect.pairwise(compare_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "nzvii9YCzXbd"
   },
   "outputs": [],
   "source": [
    "# load checklist Suite and Editor\n",
    "\n",
    "suite = TestSuite()\n",
    "editor = Editor()\n",
    "\n",
    "# load dict for data storing\n",
    "data_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "5AUHOTfCy05w"
   },
   "outputs": [],
   "source": [
    "# create data\n",
    "\n",
    "# long-range dependency spans\n",
    "\n",
    "# non-core role\n",
    "set1 = editor.template('The killer killed the victim, {mask} {mask} {mask} {mask} {mask} {mask}, last week.', meta=True, save=True)\n",
    "data_dict['span non-core'] = set1  # save to data dict for storing\n",
    "\n",
    "## core role\n",
    "set2 = editor.template('The killer killed the victim, {mask} {mask} {mask} {mask} {mask} {mask}, with a knife.', meta=True, save=True)\n",
    "data_dict['span core'] = set2\n",
    "\n",
    "# complex sentence structures\n",
    "\n",
    "## non-core role\n",
    "set3 = editor.template('The killer, {mask} {mask} {mask} {mask} {mask} {mask}, killed the victim last week.', meta=True, save=True)\n",
    "data_dict['complex non-core'] = set3\n",
    "\n",
    "## core role\n",
    "set4 = editor.template('The killer, {mask} {mask} {mask} {mask} {mask} {mask}, killed the victim with a knife.', meta=True, save=True)\n",
    "data_dict['complex core'] = set4\n",
    "\n",
    "# verb alternation\n",
    "\n",
    "## dative alternation DOC\n",
    "set5 = editor.template('{first_name1} gave {first_name2} {mask}.', nsamples=20, meta=True, save=True)\n",
    "set5 += editor.template('{first_name1} gave {first_name2} {mask} {mask}.', nsamples=20, meta=True, save=True)\n",
    "set5 += editor.template('{first_name1} gave {first_name2} {mask} {mask} {mask}.', nsamples=20, meta=True, save=True)\n",
    "set5 += editor.template('{first_name1} gave {first_name2} {mask} {mask} {mask} {mask}.', nsamples=20, meta=True, save=True)\n",
    "set5 += editor.template('{first_name1} gave {first_name2} {mask} {mask} {mask} {mask} {mask}.', nsamples=20, meta=True, save=True)\n",
    "data_dict['dative DOC'] = set5\n",
    "\n",
    "## dative alternation IOC\n",
    "set6 = editor.template('{first_name1} gave {mask} to {first_name2}.', nsamples=20, meta=True, save=True)\n",
    "set6 += editor.template('{first_name1} gave {mask} {mask} to {first_name2}.', nsamples=20, meta=True, save=True)\n",
    "set6 += editor.template('{first_name1} gave {mask} {mask} {mask} to {first_name2}.', nsamples=20, meta=True, save=True)\n",
    "set6 += editor.template('{first_name1} gave {mask} {mask} {mask} {mask} to {first_name2}.', nsamples=20, meta=True, save=True)\n",
    "set6 += editor.template('{first_name1} gave {mask} {mask} {mask} {mask} {mask} to {first_name2}.', nsamples=20, meta=True, save=True)\n",
    "data_dict['dative IOC'] = set6\n",
    "\n",
    "## causative/inchoative alternation\n",
    "set7 = editor.template(['Mary broke the {mask1}.', 'The {mask1} broke.'], nsamples=100, meta=True, save=True)\n",
    "data_dict['inchoative'] = set7\n",
    "\n",
    "# voice + robustness\n",
    "\n",
    "## voice comparison \n",
    "set8 = editor.template(['The {mask1} was written.', 'She wrote the {mask1}.'], nsamples=20, meta=True, save=True)\n",
    "set8 += editor.template(['The {mask1} was seen.', 'They saw the {mask1}.'], nsamples=20, meta=True, save=True)\n",
    "set8 += editor.template(['The {mask1} was felt.', 'I felt the {mask1}.'], nsamples=20, meta=True, save=True)\n",
    "set8 += editor.template(['The {mask1} was heard.', 'I heard the {mask1}.'], nsamples=20, meta=True, save=True)\n",
    "set8 += editor.template(['The {mask1} was created.', 'I created the {mask1}.'], nsamples=20, meta=True, save=True)\n",
    "set8_typo = Perturb.perturb(set8.data, Perturb.add_typos) # added typo's for robustness check\n",
    "data_dict['voice/robustness'] = set8\n",
    "\n",
    "# store data dict containing full challenge set as json file\n",
    "with open('challenge_set.json', 'w') as json_file:\n",
    "    json.dump(data_dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open challenge set from json file and load into dict for use\n",
    "\n",
    "with open('challenge_set.json', 'r') as f:\n",
    "    challenge_set = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize test objects\n",
    "\n",
    "# create list to store tests\n",
    "individual_tests = []\n",
    "\n",
    "# define challenge subsets from full challenge set dict\n",
    "ret1 = challenge_set['span non-core'] \n",
    "ret2 = challenge_set['span core'] \n",
    "ret3 = challenge_set['complex non-core'] \n",
    "ret4 = challenge_set['complex core'] \n",
    "ret5 = challenge_set['dative DOC'] \n",
    "ret6 = challenge_set['dative IOC'] \n",
    "ret7 = challenge_set['inchoative'] \n",
    "ret8 = challenge_set['voice/robustness']\n",
    "\n",
    "\n",
    "# long-range dependency spans\n",
    "\n",
    "# MFT non-core role\n",
    "test1 = MFT(**ret1, name = 'span non-core role', capability='Long-range dependency spans', expect=expectTMP)\n",
    "suite.add(test1, format_example_fn=format_srl_first, overwrite=True)   # add to test suite for easy running and viewing in the notebook\n",
    "individual_tests.append(test1)      # add tests to list for extracting and storing results\n",
    "\n",
    "# MFT core role\n",
    "test2 = MFT(**ret2, name = 'span core role', capability='Long-range dependency spans', expect=expect2)\n",
    "suite.add(test2, format_example_fn=format_srl_first, overwrite=True)\n",
    "individual_tests.append(test2)\n",
    "\n",
    "# complex sentence structures\n",
    "\n",
    "# MFT non-core role\n",
    "test3 = MFT(**ret3, name = 'complex non-core role', capability='Complex sentence structures', expect=expectTMP)\n",
    "suite.add(test3, format_example_fn=format_srl_last, overwrite=True)\n",
    "individual_tests.append(test3)\n",
    "\n",
    "# MFT core role\n",
    "test4 = MFT(**ret4, name = 'complex core role', capability='Complex sentence structures', expect=expect2)\n",
    "suite.add(test4, format_example_fn=format_srl_last, overwrite=True)\n",
    "individual_tests.append(test4)\n",
    "\n",
    "# verb alternation \n",
    "\n",
    "# MFT dative alternation DOC\n",
    "test5 = MFT(**ret5, name = 'Dative shift DOC', capability='Verb alternation', expect=expectDOC)\n",
    "suite.add(test5, format_example_fn=format_srl_first, overwrite=True)\n",
    "individual_tests.append(test5)\n",
    "# MFT dative alternation IOC\n",
    "test6 = MFT(**ret6, name = 'Dative shift IOC', capability='Verb alternation', expect=expectIOC)\n",
    "suite.add(test6, format_example_fn=format_srl_first, overwrite=True)\n",
    "individual_tests.append(test6)\n",
    "# DIR causative/inchoative alternation\n",
    "test7 = DIR(**ret7, name='Inchoative/causative', capability='Verb alternation', \n",
    "           description='Takes an inchoative/causative pair and checks if the generated word has the same label.', expect=expectSpan)\n",
    "suite.add(test7, format_example_fn=format_srl_first, overwrite=True)\n",
    "individual_tests.append(test7)\n",
    "\n",
    "# voice\n",
    "\n",
    "# active/passive comparison + robustness\n",
    "test8 = DIR(**ret8, name='voice comparison', capability='Verb alternation', \n",
    "           description='Takes an active/passive sentence pair and checks if the generated word has the same label.', expect=expectVoice)\n",
    "suite.add(test8, format_example_fn=format_srl_last, overwrite=True)\n",
    "individual_tests.append(test8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 100 examples\n",
      "Predicting 100 examples\n",
      "Predicting 100 examples\n",
      "Predicting 100 examples\n",
      "Predicting 100 examples\n",
      "Predicting 100 examples\n",
      "Predicting 200 examples\n",
      "Predicting 200 examples\n"
     ]
    }
   ],
   "source": [
    "# create dict to store results\n",
    "result_dict = {}\n",
    "\n",
    "# loop over each individual test, run the test, split results by 'passed' and 'failed' and save results to json file\n",
    "\n",
    "for test in individual_tests:   # loop over each test\n",
    "    passed_dict = {}\n",
    "    capability = test.capability     \n",
    "    name = test.name\n",
    "    test.run(wrapper_srl, overwrite=True)     # run test\n",
    "    \n",
    "    passed_list = list(test.results.passed)     # get pass/fail array into list\n",
    "    all_preds = list(test.results.preds)        # get all predictions\n",
    "    \n",
    "       # remove np arrays for json storing\n",
    "        \n",
    "    new_preds = []\n",
    "    for item in all_preds:\n",
    "        if isinstance(item, np.ndarray):\n",
    "            new_item = list(item)\n",
    "            new_preds.append(new_item[0])\n",
    "        else:\n",
    "            new_preds.append(item)\n",
    "    \n",
    "    true = []       # define pass and fail lists\n",
    "    false = []\n",
    "    \n",
    "    for pred, stat in zip(new_preds, passed_list):   # loop over zipped predictions \n",
    "        if stat == True:\n",
    "            true.append(pred)     # add prediction to pass or fail list\n",
    "        if stat == False:\n",
    "            false.append(pred)\n",
    "            \n",
    "    passed_dict['Passed'] = true        # add passed and failed predictions of one test to dict\n",
    "    passed_dict['Failed'] = false\n",
    "    result_dict[name] = passed_dict    # add passed dict to overall result dict with test name as key\n",
    "            \n",
    "        \n",
    "with open('test_results.json', 'w') as json_file:    # store results in json file\n",
    "    json.dump(result_dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v-Q8M221viIP",
    "outputId": "7f40bf78-ca65-4e91-ce34-8d4a0939eb34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running span non-core role\n",
      "Predicting 100 examples\n",
      "Running span core role\n",
      "Predicting 100 examples\n",
      "Running complex non-core role\n",
      "Predicting 100 examples\n",
      "Running complex core role\n",
      "Predicting 100 examples\n",
      "Running Dative shift DOC\n",
      "Predicting 100 examples\n",
      "Running Dative shift IOC\n",
      "Predicting 100 examples\n",
      "Running Inchoative/causative\n",
      "Predicting 200 examples\n",
      "Running voice comparison\n",
      "Predicting 200 examples\n",
      "\n",
      "Verb alternation\n",
      "\n",
      "Dative shift DOC\n",
      "Test cases:      100\n",
      "Fails (rate):    20 (20.0%)\n",
      "\n",
      "Example fails:\n",
      "[ARG0: Victoria] [V: gave] [ARG1: Eleanor flowers] .\n",
      "----\n",
      "[ARG0: Nancy] [V: gave] [ARG2: Victoria] [ARGM-TMP: a few minutes] .\n",
      "----\n",
      "[ARG0: Rachel] [V: gave] [ARG1: Eleanor presents] .\n",
      "----\n",
      "[ARG0: Al] [V: gave] [ARG1: Julia] away .\n",
      "----\n",
      "[ARG0: Sally] [V: gave] [ARG2: Robert CPR] .\n",
      "----\n",
      "[ARG0: Steve] [V: gave] [ARG1: Don something to try] .\n",
      "----\n",
      "[ARG0: Victoria] [V: gave] [ARG1: Eleanor \" \"] .\n",
      "----\n",
      "[ARG0: Steve] [V: gave] [ARG1: Don handcuffs] .\n",
      "----\n",
      "[ARG0: Christopher] [V: gave] [ARG1: Frances pills] .\n",
      "----\n",
      "[ARG0: Frances] [V: gave] [ARG2: Jill \" A.]\n",
      "----\n",
      "\n",
      "\n",
      "Dative shift IOC\n",
      "Test cases:      100\n",
      "Fails (rate):    21 (21.0%)\n",
      "\n",
      "Example fails:\n",
      "[ARG0: Elizabeth] [V: gave] [ARG1: it] right off [ARG2: to Sarah] .\n",
      "----\n",
      "[ARG1: Emily] [V: gave] over [ARG2: to Peter] .\n",
      "----\n",
      "[ARG0: Elizabeth] [V: gave] [ARG1: everything] [ARGM-DIR: away] [ARG2: to Sarah] .\n",
      "----\n",
      "[ARG0: Scott] [V: gave] [ARG1: it] [ARGM-DIS: right] [ARGM-LOC: along] [ARG2: to Stephen] .\n",
      "----\n",
      "Steve [V: gave] [ARG1: pills] [ARG2: to Don] .\n",
      "----\n",
      "[ARG0: Emily] [V: gave] [ARG1: it] [ARG2: right down to Peter] .\n",
      "----\n",
      "[ARG0: Alan] [V: gave] [ARG1: it all] up [ARG2: to Stephanie] .\n",
      "----\n",
      "[ARG0: Al] [V: gave] up [ARG2: to Julia] .\n",
      "----\n",
      "[ARG0: Rachel] [V: gave] [ARG1: everything] [ARG2: back to Eleanor] .\n",
      "----\n",
      "[ARG0: Sally] [V: gave] [ARG1: it] on [ARG2: to Robert] .\n",
      "----\n",
      "\n",
      "\n",
      "Inchoative/causative\n",
      "Test cases:      100\n",
      "Fails (rate):    9 (9.0%)\n",
      "\n",
      "Example fails:\n",
      "[ARG0: Mary] [V: broke] [ARG1: the first] .\n",
      "The [ARGM-TMP: first] [V: broke] .\n",
      "\n",
      "----\n",
      "[ARG0: Mary] [V: broke] [ARG1: the twins] .\n",
      "[ARG0: The twins] [V: broke] .\n",
      "\n",
      "----\n",
      "[ARG0: Mary] [V: broke] [ARG1: the first] .\n",
      "The [ARGM-TMP: first] [V: broke] .\n",
      "\n",
      "----\n",
      "[ARG0: Mary] [V: broke] [ARG1: the whip] .\n",
      "[ARG0: The whip] [V: broke] .\n",
      "\n",
      "----\n",
      "[ARG0: Mary] [V: broke] [ARG1: the fast] .\n",
      "The [ARGM-MNR: fast] [V: broke] .\n",
      "\n",
      "----\n",
      "[ARG0: Mary] [V: broke] [ARG1: the twins] .\n",
      "[ARG0: The twins] [V: broke] .\n",
      "\n",
      "----\n",
      "[ARG0: Mary] [V: broke] [ARG1: the first] .\n",
      "The [ARGM-TMP: first] [V: broke] .\n",
      "\n",
      "----\n",
      "[ARG0: Mary] [V: broke] [ARG1: the Law] .\n",
      "[ARG0: The Law] [V: broke] .\n",
      "\n",
      "----\n",
      "[ARG0: Mary] [V: broke] [ARG1: the first] .\n",
      "The [ARGM-TMP: first] [V: broke] .\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "voice comparison\n",
      "Test cases:      100\n",
      "Fails (rate):    2 (2.0%)\n",
      "\n",
      "Example fails:\n",
      "The horn [V: was] heard .\n",
      "[ARG0: I] [V: heard] [ARG1: the horn] .\n",
      "\n",
      "----\n",
      "The gunshot [V: was] heard .\n",
      "[ARG0: I] [V: heard] [ARG1: the gunshot] .\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Long-range dependency spans\n",
      "\n",
      "span non-core role\n",
      "Test cases:      100\n",
      "Fails (rate):    96 (96.0%)\n",
      "\n",
      "Example fails:\n",
      "[ARG0: The killer] [V: killed] [ARG1: the victim , who would have just turned 20 , last week] .\n",
      "----\n",
      "[ARG0: The killer] [V: killed] [ARG1: the victim , who passed away early Tuesday morning , last week] .\n",
      "----\n",
      "[ARG0: The killer] [V: killed] [ARG1: the victim , who was pregnant with a newborn , last week] .\n",
      "----\n",
      "[ARG0: The killer] [V: killed] [ARG1: the victim , who was pregnant with a toddler , last week] .\n",
      "----\n",
      "[ARG0: The killer] [V: killed] [ARG1: the victim , who was pregnant with their son , last week] .\n",
      "----\n",
      "[ARG0: The killer] [V: killed] [ARG1: the victim , who did not disclose his age , last week] .\n",
      "----\n",
      "[ARG0: The killer] [V: killed] [ARG1: the victim , who did not have an attorney , last week] .\n",
      "----\n",
      "[ARG0: The killer] [V: killed] [ARG1: the victim , who passed away earlier this week , last week] .\n",
      "----\n",
      "[ARG0: The killer] [V: killed] [ARG1: the victim , who did not say a name , last week] .\n",
      "----\n",
      "[ARG0: The killer] [V: killed] [ARG1: the victim , who later died at her home , last week] .\n",
      "----\n",
      "\n",
      "\n",
      "span core role\n",
      "Test cases:      100\n",
      "Fails (rate):    100 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "[ARG0: The killer] [V: killed] [ARG1: the victim , identified only by his last name , with a knife] .\n",
      "----\n",
      "[ARG0: The killer] [V: killed] [ARG1: the victim , who was pregnant at that point , with a knife] .\n",
      "----\n",
      "[ARG0: The killer] [V: killed] [ARG1: the victim , who was bleeding to the death , with a knife] .\n",
      "----\n",
      "[ARG0: The killer] [V: killed] [ARG1: the victim , who was covered in human blood , with a knife] .\n",
      "----\n",
      "[ARG0: The killer] [V: killed] [ARG1: the victim , identified only by his first name , with a knife] .\n",
      "----\n",
      "[ARG0: The killer] [V: killed] [ARG1: the victim , who was in her early twenties , with a knife] .\n",
      "----\n",
      "[ARG0: The killer] [V: killed] [ARG1: the victim , who was struggling for her life , with a knife] .\n",
      "----\n",
      "[ARG0: The killer] [V: killed] [ARG1: the victim , who was bleeding from the stomach , with a knife] .\n",
      "----\n",
      "[ARG0: The killer] [V: killed] [ARG1: the victim , who was covered by a sheet , with a knife] .\n",
      "----\n",
      "[ARG0: The killer] [V: killed] [ARG1: the victim , who died at a nearby hospital , with a knife] .\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Complex sentence structures\n",
      "\n",
      "complex non-core role\n",
      "Test cases:      100\n",
      "Fails (rate):    99 (99.0%)\n",
      "\n",
      "Example fails:\n",
      "[ARG0: The killer] , [R-ARG0: who] [V: pleaded] [ARG1: not guilty] [ARGM-TMP: last October] , killed the victim last week .\n",
      "----\n",
      "[ARG0: The killer] , [R-ARG0: who] [V: pleaded] [ARG1: not guilty pending sentencing] , killed the victim last week .\n",
      "----\n",
      "[ARG0: The killer] , [R-ARG0: who] [V: pleaded] [ARG1: not guilty] [ARGM-TMP: this year] , killed the victim last week .\n",
      "----\n",
      "[ARG0: The killer] , [R-ARG0: who] [V: pleaded] [ARG1: not guilty to robbery] , killed the victim last week .\n",
      "----\n",
      "[ARG0: The killer] , [R-ARG0: who] [V: pleaded] [ARG1: not guilty during sentencing] , killed the victim last week .\n",
      "----\n",
      "[ARG0: The killer] , [R-ARG0: who] [V: pleaded] [ARG1: not guilty to assault] , killed the victim last week .\n",
      "----\n",
      "[ARG0: The killer] , [R-ARG0: who] [V: pleaded] [ARG1: not guilty] [ARGM-TMP: on Friday] , killed the victim last week .\n",
      "----\n",
      "[ARG0: The killer] , [R-ARG0: who] [V: pleaded] [ARG1: not guilty] [ARGM-TMP: this spring] , killed the victim last week .\n",
      "----\n",
      "[ARG0: The killer] , [R-ARG0: who] [V: pleaded] [ARG1: not guilty] [ARGM-TMP: on Thursday] , killed the victim last week .\n",
      "----\n",
      "[ARG0: The killer] , [R-ARG0: who] [V: pleaded] [ARG1: not guilty] [ARGM-TMP: in 2010] , killed the victim last week .\n",
      "----\n",
      "\n",
      "\n",
      "complex core role\n",
      "Test cases:      100\n",
      "Fails (rate):    100 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "The killer , who [V: did] not have a lawyer , killed the victim with a knife .\n",
      "----\n",
      "The killer , who [V: does] not have a weapon , killed the victim with a knife .\n",
      "----\n",
      "The killer , who [V: does] not have a name , killed the victim with a knife .\n",
      "----\n",
      "[ARG1: The killer] , [R-ARG1: who] [V: goes] [ARGM-MNR: only by her surname] , killed the victim with a knife .\n",
      "----\n",
      "The killer , who [V: does] not have criminal record , killed the victim with a knife .\n",
      "----\n",
      "The killer , who [V: did] not disclose his name , killed the victim with a knife .\n",
      "----\n",
      "The killer , who [V: does] not have a cellphone , killed the victim with a knife .\n",
      "----\n",
      "The killer , who [V: did] not possess a gun , killed the victim with a knife .\n",
      "----\n",
      "[ARG1: The killer] , [R-ARG1: who] [V: goes] [ARG4: only under his alias] , killed the victim with a knife .\n",
      "----\n",
      "The killer , who [V: did] not have a cellphone , killed the victim with a knife .\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run suite LSTM for an overview in the notebook or look at the json file test_results.json\n",
    "\n",
    "suite.run(wrapper_srl, overwrite=True)\n",
    "print()\n",
    "suite.summary(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running span non-core role\n",
      "Predicting 100 examples\n",
      "Running span core role\n",
      "Predicting 100 examples\n",
      "Running complex non-core role\n",
      "Predicting 100 examples\n",
      "Running complex core role\n",
      "Predicting 100 examples\n",
      "Running Dative shift DOC\n",
      "Predicting 100 examples\n",
      "Running Dative shift IOC\n",
      "Predicting 100 examples\n",
      "Running Inchoative/causative\n",
      "Predicting 200 examples\n",
      "Running voice comparison\n",
      "Predicting 200 examples\n",
      "\n",
      "Verb alternation\n",
      "\n",
      "Dative shift DOC\n",
      "Test cases:      100\n",
      "Fails (rate):    14 (14.0%)\n",
      "\n",
      "Example fails:\n",
      "[ARG0: Sally] [V: gave] [ARG2: Robert CPR] .\n",
      "----\n",
      "[ARG0: Christopher] [V: gave] [ARG2: Frances] [ARG1: a kiss] [ARGM-LOC: on the table] .\n",
      "----\n",
      "[ARG0: Sara] [V: gave] [ARG2: Anthony] [ARG1: a kiss] [ARGM-LOC: on the porch] .\n",
      "----\n",
      "\n",
      "\n",
      "Dative shift IOC\n",
      "Test cases:      100\n",
      "Fails (rate):    20 (20.0%)\n",
      "\n",
      "Example fails:\n",
      "[ARG0: Al] [V: gave] up [ARG1: to Julia] .\n",
      "----\n",
      "[ARG0: Jay] [V: gave] [ARG1: it all] [ARGM-MNR: straight] [ARG2: to Donald] .\n",
      "----\n",
      "[ARG0: Christopher] [V: gave] [ARG1: it] out [ARG2: to Frances] .\n",
      "----\n",
      "\n",
      "\n",
      "Inchoative/causative\n",
      "Test cases:      100\n",
      "Fails (rate):    0 (0.0%)\n",
      "\n",
      "\n",
      "voice comparison\n",
      "Test cases:      100\n",
      "Fails (rate):    0 (0.0%)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Long-range dependency spans\n",
      "\n",
      "span non-core role\n",
      "Test cases:      100\n",
      "Fails (rate):    18 (18.0%)\n",
      "\n",
      "Example fails:\n",
      "[ARG0: The killer] [V: killed] [ARG1: the victim , who passed away earlier this summer , last week] .\n",
      "----\n",
      "[ARG0: The killer] [V: killed] [ARG1: the victim , who passed away early this year , last week] .\n",
      "----\n",
      "[ARG0: The killer] [V: killed] [ARG1: the victim , who died at a nearby hospital , last week] .\n",
      "----\n",
      "\n",
      "\n",
      "span core role\n",
      "Test cases:      100\n",
      "Fails (rate):    2 (2.0%)\n",
      "\n",
      "Example fails:\n",
      "[ARG0: The killer] [V: killed] [ARG1: the victim , who described herself as an atheist] , [ARGM-MNR: with a knife] .\n",
      "----\n",
      "[ARG0: The killer] [V: killed] [ARG1: the victim , who described herself as an engineer , with a knife] .\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Complex sentence structures\n",
      "\n",
      "complex non-core role\n",
      "Test cases:      100\n",
      "Fails (rate):    99 (99.0%)\n",
      "\n",
      "Example fails:\n",
      "[ARG0: The killer] , [R-ARG0: who] [V: pleaded] [ARG1: not guilty] [ARG2: to murder] , killed the victim last week .\n",
      "----\n",
      "[ARG0: The killer] , [R-ARG0: who] [V: pleaded] [ARG1: not guilty] [ARGM-TMP: in 2013] , killed the victim last week .\n",
      "----\n",
      "[ARG0: The killer] , [R-ARG0: who] [V: pleaded] [ARG1: not guilty] [ARGM-TMP: on Tuesday] , killed the victim last week .\n",
      "----\n",
      "\n",
      "\n",
      "complex core role\n",
      "Test cases:      100\n",
      "Fails (rate):    100 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "[ARG0: The killer] , [R-ARG0: who] [V: goes] [ARGM-MNR: only by one pseudonym] , killed the victim with a knife .\n",
      "----\n",
      "[ARG0: The killer] , [R-ARG0: who] [V: goes] [ARGM-MNR: only by his pseudonym] , killed the victim with a knife .\n",
      "----\n",
      "The killer , who [V: did] not have a cellphone , killed the victim with a knife .\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run test suite bert for an overview in the notebook or look at the json file test_results.json\n",
    "\n",
    "suite.run(wrapper_bert, overwrite=True)\n",
    "print()\n",
    "suite.summary(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "intermediate_CheckList_SRL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
